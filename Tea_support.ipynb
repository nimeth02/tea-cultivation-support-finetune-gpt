{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1pyuhh0zKXnxu51EjwzN06QP6swAy4h09",
      "authorship_tag": "ABX9TyNKh5vzrFBeGYFGgxFSrRuj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nimeth02/tea-cultivation-support-finetune-gpt/blob/main/Tea_support.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whYPmbI3nLiK",
        "outputId": "537fde2a-e8d1-4eb7-d97d-ad8c2eeb624f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q torch transformers sentencepiece accelerate bitsandbytes datasets peft"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301,
          "referenced_widgets": [
            "abb3b7eb0a1a4a6c9497e3220ab16638",
            "dd392be62558463fbc23133a854a5208",
            "f9bfef764ac1448bbcd81d2d9cfb9481",
            "2dcc014f41114915a4484cc5ce7e0e24",
            "dc6d92e7297f436aa40b3418d95cebb3",
            "ebf4e6c7dd434990adf7a447eb4aaf1e",
            "4631006f6d5e41a68bee20589af6ed7f",
            "285d02d59f3c4d6fad1e5a7373329a95",
            "9170ccf7b94d41e2b43a9fc65fd81b58",
            "5be6c7f3e82d422398a301b8736dcfc1",
            "1ea43ac479b547a7b3bf3b9192a9e2fa",
            "08bd19f87d1842629d53f02c9222355c",
            "f9e5b144379b426b9a1cba6a8f3f0856",
            "476a7b707c5a4bf3aed8c45963b7d2ea",
            "df60f73de53048318448621169fca9ff",
            "8d5bf0c55e01486bb86b93bb9d274cff",
            "901784e74d8347a4b1b26210b0dd24f2",
            "07dd466ac67d43c0b069e387057b8930",
            "b6034ee0e0fc4cde8e338a720522599a",
            "dce482796f7c473ab74a1ebfb6801c11",
            "be14c41633064742b6b0c71f8008397c",
            "907cc34169904831bd38926ace2eaddc",
            "4e20440d04eb4d06ab490bf7616662e2",
            "fc5fd1160ca74b4ea07efe6caa7e0e0b",
            "f8e040c7e8fa417c961150ad0959e1d7",
            "4f995351cdc3446c885cf3f5c7c4c6da",
            "834f44afcfd840249073b8ea804f907f",
            "26063f1a4df94a7eb8c8fd179f76b620",
            "7023554da4564e319d71d3f9c399e257",
            "16a79e8722434ca790f02d139f5d5e18",
            "7cea2201d259451ebccfabc7e6a5a81e",
            "7841d9b3faa34aeb9acc639f33ef0e8b",
            "d4edd7be87a94c46a5248551682986c3",
            "7a388d11fd804d89b74a2645466bdb82",
            "f4eee69b945347cf8d4373eebafb7a61",
            "8754fc306b814643b8cc7a3d29e4b219",
            "cb3b1ee1ab6b426f93b876f673bce0d2",
            "600f8bb8a8524db397fae1467f5bb08e",
            "20b27f29aa634fab9960b2999edc60e5",
            "3621a369484d4d849317cde5b401289c",
            "3689f0692a85482eab868c2d70a114fd",
            "ff0c9579c7e2443c80bcca86c7ae28cf",
            "4fcdf76ce861404da0d8e35dd79dbb28",
            "d1aa0d2f50504abf93abae1ea67cb487",
            "6cd599d74e704b7faeb05c2492b99af9",
            "447f255b671b4077a2b4bac5da006200",
            "eb4f5d34ca514663aff30ffac6331c75",
            "b1889215172f40b69a5328b31a158d06",
            "0a219b217b284d3682fa06385e23e75e",
            "fc3783f4c03d4ac9a7e9b7c73b8a65bd",
            "80d61d846c1b4f4eb0c6344e014938c8",
            "3fc816fe019948378eed9be75789f9cf",
            "a4cfb2a80d96457b95f51755dceca6b1",
            "9074c570fcea4550892fbab03b154660",
            "4265399e3dde41218c6d88de83edafb4"
          ]
        },
        "id": "hgNEohiie-MB",
        "outputId": "d83c422d-5d80-453c-9dc0-f754de61459b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "abb3b7eb0a1a4a6c9497e3220ab16638"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "08bd19f87d1842629d53f02c9222355c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4e20440d04eb4d06ab490bf7616662e2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7a388d11fd804d89b74a2645466bdb82"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6cd599d74e704b7faeb05c2492b99af9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from datasets import load_dataset, Dataset\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "# Initialize tokenizer (needed for tokenization step)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove special characters, keep alphanumeric and basic punctuation\n",
        "    text = re.sub(r'[^a-z0-9\\s.,!?%-]', '', text)\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    # Remove very short or empty lines (e.g., less than 3 characters)\n",
        "    if len(text) < 3:\n",
        "        return None\n",
        "    return text\n",
        "\n",
        "# Load plain text\n",
        "with open(\"text_all.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# Apply preprocessing and filter out None values\n",
        "cleaned_lines = [preprocess_text(line) for line in lines]\n",
        "cleaned_lines = [line for line in cleaned_lines if line is not None]\n",
        "\n",
        "# Create HuggingFace dataset\n",
        "dataset = Dataset.from_dict({\"text\": cleaned_lines})\n",
        "\n",
        "# Tokenize\n",
        "def tokenize(example):\n",
        "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize, batched=True)"
      ],
      "metadata": {
        "id": "wb9sO7FMWR4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token  # GPT2 has no pad token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Apply LoRA for parameter-efficient tuning\n",
        "lora_config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"c_attn\"],  # Key module in GPT2\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "\n",
        "from peft import get_peft_model\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124,
          "referenced_widgets": [
            "cb5c1efeb05847a6bde6adaa18451693",
            "d6e5c55ef53a4130b10e9495542fb078"
          ]
        },
        "id": "WcsEWRL2kO8_",
        "outputId": "2eca2059-b909-4478-bca9-08b1776acb60"
      },
      "execution_count": 4,
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cb5c1efeb05847a6bde6adaa18451693",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d6e5c55ef53a4130b10e9495542fb078",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 1,179,648 || all params: 125,619,456 || trainable%: 0.9391\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/layer.py:1803: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/gpt2-agri-lora\",\n",
        "    overwrite_output_dir=True,\n",
        "    per_device_train_batch_size=2,\n",
        "    num_train_epochs=3,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    prediction_loss_only=True,\n",
        "    logging_steps=100,\n",
        "    fp16=True,  # If supported\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=False  # GPT2 uses causal LM\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGxTUmKiA1aP",
        "outputId": "aaf9c1da-9b45-4d1e-879c-30639eb3c7d1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-5-4193002023.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "frDcVEGyA5ml",
        "outputId": "3031d803-7934-429d-f7b5-d434530b2849"
      },
      "execution_count": 6,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='68' max='765' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 68/765 00:06 < 01:11, 9.72 it/s, Epoch 0.26/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='765' max='765' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [765/765 01:23, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>5.592400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>5.299700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>5.152000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>5.186700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>5.008900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>5.033500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>5.002600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=765, training_loss=5.168509848600899, metrics={'train_runtime': 85.0165, 'train_samples_per_second': 17.996, 'train_steps_per_second': 8.998, 'total_flos': 405321343303680.0, 'train_loss': 5.168509848600899, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./gpt2-tea\")\n",
        "tokenizer.save_pretrained(\"./gpt2-tea\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDgMS19USWx0",
        "outputId": "7185f8f1-50eb-4e99-d027-8e9caf68dcc6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./gpt2-tea/tokenizer_config.json',\n",
              " './gpt2-tea/special_tokens_map.json',\n",
              " './gpt2-tea/vocab.json',\n",
              " './gpt2-tea/merges.txt',\n",
              " './gpt2-tea/added_tokens.json',\n",
              " './gpt2-tea/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, AutoTokenizer, pipeline\n",
        "import torch\n",
        "\n",
        "def format_with_linebreaks(text: str) -> str:\n",
        "    \"\"\"Cleans up line breaks in generated text.\"\"\"\n",
        "    return \"\\n\".join(line.strip() for line in text.split(\"\\n\") if line.strip())\n",
        "\n",
        "# Function to generate and format responses\n",
        "def generate_response(model, tokenizer, prompt):\n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "            repetition_penalty=1.2,       # Reduce repetition\n",
        "    temperature=0.7,              # Balanced creativity\n",
        "    top_k=40,                     # Avoid weird word choices\n",
        "    top_p=0.9,                    # Smarter sampling\n",
        "    do_sample=True,\n",
        "        device=0 if torch.cuda.is_available() else -1\n",
        "    )\n",
        "    output = pipe(prompt, max_length=50, num_return_sequences=1)\n",
        "    return format_with_linebreaks(output[0]['generated_text'])\n",
        "\n",
        "# Initialize both models\n",
        "base_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "base_tokenizer.pad_token_id = base_tokenizer.eos_token_id\n",
        "base_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "custom_tokenizer = AutoTokenizer.from_pretrained(\"./gpt2-tea\")\n",
        "custom_tokenizer.pad_token_id = custom_tokenizer.eos_token_id\n",
        "custom_model = GPT2LMHeadModel.from_pretrained(\"./gpt2-tea\")\n",
        "\n",
        "# Test prompt\n",
        "prompt = \"What is the recommended spacing for tea bushes?\"\n",
        "\n",
        "# Generate responses\n",
        "print(\"=== Base GPT-2 ===\")\n",
        "print(generate_response(base_model, base_tokenizer, prompt))\n",
        "\n",
        "print(\"\\n=== Fine-tuned Model ===\")\n",
        "print(generate_response(custom_model, custom_tokenizer, prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNbrsYb6PMBE",
        "outputId": "8be4aeb8-14e0-4356-b0dd-d4a5b17b24e7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Base GPT-2 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Device set to use cuda:0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is the recommended spacing for tea bushes?\n",
            "This topic will be covered in more detail later on. The recommendation should not affect your overall taste buds, so you can get what's best at a reasonable price and use it as needed to keep yourself going when buying new teas (or even better!).\n",
            "\n",
            "=== Fine-tuned Model ===\n",
            "What is the recommended spacing for tea bushes?\n",
            "The optimum planting time to plant depends on various factors such as your climate and soil conditions. The best technique may vary depending upon which type of tree species you choose, but most importantly it can help protect a variety from damage caused by pests or disease in particular climates when compared with conventional methods where many plants are planted only once per year during spring/summer season while others need additional watering every other day due not being able take advantage at times that they cannot be found outside their range (e:g., overhanging trees). Therefore any individual cultivar should have an optimal spaced location within its reach if desired; however this does require careful planning because some varieties will grow taller than those intended so there might be more strain applied throughout one's growing period whereas another could still yield better yields under different climatic circumstances based solely on crop rotation rates alone. This process usually involves several months each month followed closely around summertime through early fall after flowering seeds begin ripening along wt-30% tolerance till date. For these purposes teas often tend towards shorter growth cycles leading up into winter rather then higher temperatures lasting longer periods whilst increasing production capacity further increases productivity resulting in faster turnover rate across much less productive areas like nurseries, vineyards etc. If required specific advice regarding tim\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sjxIrULZVkxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./gpt2-tea\")"
      ],
      "metadata": {
        "id": "wqWD510S_OOA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "\n",
        "# Load base model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Load domain adapter (previously saved)\n",
        "model = PeftModel.from_pretrained(base_model, \"./gpt2-tea\")"
      ],
      "metadata": {
        "id": "a5maNek1_STP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "sft_lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"c_attn\", \"c_proj\", \"mlp.c_fc\", \"mlp.c_proj\"],  # same as before or adjusted\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "\n",
        "# Add the new LoRA adapter on top of the domain-adapted model\n",
        "model = get_peft_model(model, sft_lora_config)\n",
        "model.print_trainable_parameters()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1fo1-b1_U1N",
        "outputId": "9b61cc48-2725-48b7-d5e7-25e964bc6049"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 1,179,648 || all params: 125,619,456 || trainable%: 0.9391\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/layer.py:1803: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the uploaded CSV\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/tea_chat_data.csv\")\n",
        "\n",
        "# Show the first few rows (optional)\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnORvQ___bsC",
        "outputId": "da588d98-70cd-4cb4-c9d2-6ca3a62edd4e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            Question  \\\n",
            "0  How should I prepare land for tea cultivation ...   \n",
            "1  What pests commonly affect tea plantations in ...   \n",
            "2  What is the ideal climate for tea cultivation ...   \n",
            "3  How is soil fertility maintained in tea planta...   \n",
            "4    What is the recommended spacing for tea bushes?   \n",
            "\n",
            "                                              Answer Unnamed: 2  \n",
            "0  Clear vegetation, remove rocks, and fork the s...        NaN  \n",
            "1  Common pests include tea tortrix, shot-hole bo...        NaN  \n",
            "2  Tea thrives with plenty of rain and stable yea...        NaN  \n",
            "3  Soil fertility is maintained through regular f...        NaN  \n",
            "4  The recommended spacing is 1.2 meters between ...        NaN  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new \"text\" column for training\n",
        "df[\"text\"] = \"Q: \" + df[\"Question\"] + \"\\nA: \" + df[\"Answer\"]\n"
      ],
      "metadata": {
        "id": "kw262vZBBKJf"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "dataset = Dataset.from_pandas(df)\n"
      ],
      "metadata": {
        "id": "A49drjdMBQQ3"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_fn(example):\n",
        "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_fn, remove_columns=dataset.column_names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "a145aad38476450db7944303ec5c81be",
            "5ccf90266f374afd848d27e96d954f3a",
            "8a31b9a2f7eb402daae5cbb21d542f44",
            "9352bc9e76994c39bd7100827514e5b3",
            "a0d3762686de4f1cbb95460c17e31903",
            "60e7350bfc3f4f3da34be6b094a96197",
            "f7aed6492aad4260a3849db4c57d2f78",
            "ca9b4c46eb234057804c3378fd67ed99",
            "41f7d522ac8e446eba451dba97e6be0e",
            "42b5d19cc68b407dbb79e1e7667e39a1",
            "aca38635632a40788d313b5f667f1725"
          ]
        },
        "id": "FmgKJ6R0BSvf",
        "outputId": "612e643e-03ec-40ea-8265-79cc27c42ede"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/361 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a145aad38476450db7944303ec5c81be"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-tea-sft\",\n",
        "    overwrite_output_dir=True,\n",
        "    per_device_train_batch_size=2,\n",
        "    num_train_epochs=3,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=100,\n",
        "    prediction_loss_only=True,\n",
        "    fp16=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "w68lQVgr_cLI",
        "outputId": "248e68ba-fdc9-4ac2-9cda-1d017158fa6b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-27-388193443.py:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='543' max='543' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [543/543 01:40, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>3.637000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>3.021100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>2.763500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>2.623400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>2.562100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=543, training_loss=2.887635050118518, metrics={'train_runtime': 100.3414, 'train_samples_per_second': 10.793, 'train_steps_per_second': 5.412, 'total_flos': 71725982810112.0, 'train_loss': 2.887635050118518, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./gpt2-tea-sft-adapter-m\")\n",
        "tokenizer.save_pretrained(\"./gpt2-tea-sft-adapter-m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s15iJTbi_ffu",
        "outputId": "44229b50-9e3c-4ea7-e1bc-4b8118347720"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./gpt2-tea-sft-adapter-m/tokenizer_config.json',\n",
              " './gpt2-tea-sft-adapter-m/special_tokens_map.json',\n",
              " './gpt2-tea-sft-adapter-m/vocab.json',\n",
              " './gpt2-tea-sft-adapter-m/merges.txt',\n",
              " './gpt2-tea-sft-adapter-m/added_tokens.json',\n",
              " './gpt2-tea-sft-adapter-m/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, AutoTokenizer, pipeline\n",
        "import torch\n",
        "\n",
        "def format_with_linebreaks(text: str) -> str:\n",
        "    \"\"\"Cleans up line breaks in generated text.\"\"\"\n",
        "    return \"\\n\".join(line.strip() for line in text.split(\"\\n\") if line.strip())\n",
        "\n",
        "# Function to generate and format responses\n",
        "def generate_response(model, tokenizer, prompt):\n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "            repetition_penalty=1.2,       # Reduce repetition\n",
        "    temperature=0.7,              # Balanced creativity\n",
        "    top_k=40,                     # Avoid weird word choices\n",
        "    top_p=0.9,                    # Smarter sampling\n",
        "    do_sample=True,\n",
        "        device=0 if torch.cuda.is_available() else -1\n",
        "    )\n",
        "    output = pipe(prompt, max_length=20, num_return_sequences=1)\n",
        "    return format_with_linebreaks(output[0]['generated_text'])\n",
        "\n",
        "# Initialize both models\n",
        "base_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "base_tokenizer.pad_token_id = base_tokenizer.eos_token_id\n",
        "base_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "_custom_tokenizer = AutoTokenizer.from_pretrained(\"./gpt2-tea\")\n",
        "_custom_tokenizer.pad_token_id = custom_tokenizer.eos_token_id\n",
        "_custom_model = GPT2LMHeadModel.from_pretrained(\"./gpt2-tea\")\n",
        "\n",
        "custom_model = PeftModel.from_pretrained(_custom_model, \"./gpt2-tea-sft-adapter-m\")\n",
        "custom_tokenizer.pad_token_id = custom_tokenizer.eos_token_id\n",
        "custom_model = GPT2LMHeadModel.from_pretrained(\"./gpt2-tea\")\n",
        "# Test prompt\n",
        "prompt = \"How should I prepare land for tea cultivation?\"\n",
        "\n",
        "# Generate responses\n",
        "print(\"=== Base GPT-2 ===\")\n",
        "print(generate_response(base_model, base_tokenizer, prompt))\n",
        "\n",
        "print(\"\\n=== Fine-tuned Model ===\")\n",
        "print(generate_response(custom_model, custom_tokenizer, prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTEOa8FzBIkf",
        "outputId": "99a92830-2b05-4a4c-9f59-6745d81691cc"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/peft_model.py:585: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.transformer.h.0.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.0.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.0.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.0.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.0.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.0.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.0.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.0.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.1.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.1.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.1.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.1.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.1.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.1.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.1.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.1.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.2.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.2.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.2.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.2.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.2.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.2.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.2.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.2.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.3.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.3.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.3.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.3.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.3.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.3.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.3.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.3.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.4.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.4.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.4.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.4.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.4.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.4.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.4.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.4.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.5.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.5.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.5.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.5.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.5.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.5.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.5.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.5.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.6.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.6.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.6.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.6.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.6.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.6.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.6.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.6.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.7.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.7.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.7.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.7.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.7.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.7.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.7.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.7.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.8.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.8.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.8.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.8.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.8.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.8.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.8.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.8.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.9.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.9.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.9.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.9.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.9.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.9.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.9.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.9.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.10.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.10.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.10.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.10.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.10.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.10.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.10.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.10.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.11.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.11.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.11.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.11.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.11.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.11.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.11.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.11.mlp.c_proj.lora_B.default.weight'].\n",
            "  warnings.warn(warn_message)\n",
            "Device set to use cuda:0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Base GPT-2 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How should I prepare land for tea cultivation?\n",
            "The reason that people are interested in how to make a proper soil is because it provides an opportunity of cultivating. In the past, we have cultivated fields on earth and then brought them back with us when our crops were exhausted or lost their fruit capacity (i e., they never recovered). So why do farmers think about this situation now?\" \"But what can be done by using water instead?!\" After all, you cannot cultivate at ground level if your crop has already been drained down into sand! Therefore there will only need one thing: use irrigation system as well.\" He paused while he said these words… The first step was actually making sure not too much rain would fall over his field area so even though some rainfall might happen but no more than two inches per year could cover him just fine once again without any further rains falling due to poor drainage systems like ditches etc. However after looking up from pondering whether such problems existed within agriculture anymore before asking Zhang Xiaohua who had asked me earlier which type of farming method did best….he replied : It depends upon where my country lies…..So here's mine – 1) Water-based methods 2a) Using fresh fruits 3b] Use fertiliser 4c), No watering 5d)—If drought occurs 6e—\n",
            "\n",
            "=== Fine-tuned Model ===\n",
            "How should I prepare land for tea cultivation?\n",
            "The soil and water of the area will vary depending on what types of plants are growing, how much moisture is being released by plant growth (e.g., drought), whether or not they need to be watered in order that their leaves can grow well beyond normal leaf diameter limits, as per usual with conventional agriculture methods such a medium-sized pot yields up about 30% more nutrients than an average large size pots; however when it comes down into these critical areas where temperatures reach 65 degrees Celsius there may even be problems during planting which could lead growers away from traditional practices altogether because this heat also creates air pollution – especially due mostly nitrogen dioxide emissions resulting directly through evaporation processes but often indirectly via groundwater contamination). A variety has been proposed including various forms containing soluble fibre used mainly at high levels throughout India like 'dharana' made using organic cotton fibres grown under low temperature conditions without any loss of carbonation properties found elsewhere while other varieties include bamboo bark/cotton seedlings derived from small amounts thereof known collectively as lumbricax rootts. These perennial crops produce excellent yield rates despite having no significant impact on climate change associated crop failure so far though most have shown little effect upon global warming impacts since some examples use less CO 2 comparedto others whereas many contain\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "model.push_to_hub(\n",
        "    \"nimeth02/tea-gpt2-lora-sft-m\",\n",
        "    custom_tokenizer,\n",
        "    token = userdata.get('HF_ACCESS_TOKEN')\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "a8dcdb2667824750bb9cfeaf9e59f1cf",
            "edb08c2a7a9d41c69d964b90d3f6ae05",
            "29a10ebf335d488285146c10a61e3376",
            "d12a520c24ad4726b2c0fb5b1968be29",
            "d994cf27e04b457db2d213387914e1b1",
            "db121abc71b24336b6b5ee92980289e9",
            "f399f87d75244906aedab5d73329aeef",
            "e74aec0e6f6c407588c68b4fbb5b558b",
            "a8e3b4e1f8e348b2bc2ab8f59adefc71",
            "2af86d10ab8d4acba3c510d517c41240",
            "f17f31ea6f7a4c65b0350503dfacac5a"
          ]
        },
        "id": "I0Y-DG2miznn",
        "outputId": "dadd9b3e-1940-4ed8-a70f-d07a586fba1e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/4.73M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a8dcdb2667824750bb9cfeaf9e59f1cf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/nimeth02/tea-gpt2-lora-sft-m/commit/8e8c71a7824420945d652a301d20301263f8ef9f', commit_message='Upload model', commit_description='', oid='8e8c71a7824420945d652a301d20301263f8ef9f', pr_url=None, repo_url=RepoUrl('https://huggingface.co/nimeth02/tea-gpt2-lora-sft-m', endpoint='https://huggingface.co', repo_type='model', repo_id='nimeth02/tea-gpt2-lora-sft-m'), pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "# Replace this with your model repo ID on Hugging Face\n",
        "repo_id = \"nimeth02/tea-gpt2-lora-sft\"\n",
        "\n",
        "config = PeftConfig.from_pretrained(repo_id, token=userdata.get('HF_ACCESS_TOKEN'))"
      ],
      "metadata": {
        "id": "o_YxvC0QLpxU"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model_name = \"gpt2\"  # Replace with the actual base model if different\n",
        "\n",
        "# Load base model and tokenizer\n",
        "base_model = AutoModelForCausalLM.from_pretrained(base_model_name, token=hf_token)\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name, token=hf_token)\n"
      ],
      "metadata": {
        "id": "fY4TCa1DLvv0"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "   # or just paste it directly as string\n",
        "\n",
        "# Load PEFT adapter config\n",
        "\n",
        "\n",
        "# Load base model (GPT-2) and tokenizer\n",
        "\n",
        "# Load LoRA adapter on top of base model\n",
        "model = PeftModel.from_pretrained(base_model, repo_id, token=userdata.get('HF_ACCESS_TOKEN'))\n",
        "\n",
        "# Optional: Merge LoRA weights if you're not training further\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Enable streaming output\n",
        "streamer = TextStreamer(tokenizer)\n",
        "\n",
        "# Tokenize input\n",
        "prompt = \"How should I prepare land for tea cultivation?\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Generate\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    streamer=streamer,\n",
        "    max_new_tokens=100,\n",
        "    do_sample=True,\n",
        "    temperature=0.8,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=1.2,  # Helps stop loops\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "\n",
        "# If you want full text:\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBS5xBXkIJBq",
        "outputId": "daa96138-a5fb-4301-8758-8e7d18f8233c"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/layer.py:1803: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/peft_model.py:585: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.transformer.h.0.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.0.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.1.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.1.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.2.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.2.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.3.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.3.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.4.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.4.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.5.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.5.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.6.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.6.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.7.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.7.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.8.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.8.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.9.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.9.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.10.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.10.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.11.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.11.attn.c_attn.lora_B.default.weight'].\n",
            "  warnings.warn(warn_message)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How should I prepare land for tea cultivation?\n",
            "A lot. There are a couple of different ways to gather the necessary materials and cultivate it, but there is no way in hell you can actually make your own soil from all these other resources alone! It's almost impossible (unless one uses an old technique), so even if they could just collect those seeds themselves instead of using anything else on their farms or at home…. Even more ridiculous than that would be cultivating trees with only two leaves per leaf… This makes me really curious about how many plants\n",
            "How should I prepare land for tea cultivation?\n",
            "A lot. There are a couple of different ways to gather the necessary materials and cultivate it, but there is no way in hell you can actually make your own soil from all these other resources alone! It's almost impossible (unless one uses an old technique), so even if they could just collect those seeds themselves instead of using anything else on their farms or at home…. Even more ridiculous than that would be cultivating trees with only two leaves per leaf… This makes me really curious about how many plants\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vt6KsO32LGg3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
